<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="InterLinK: Visual Recognition and Anticipation of Human-Object Interactions using Deep Learning, Knowledge Graphs and Reasoning">
  <meta name="keywords" content="Action anticipation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VLMAH: Visual-Linguistic Modeling of Action History
for Effective Action Anticipation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VLMAH: Visual-Linguistic Modeling of Action History
for Effective Action Anticipation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="http://users.ics.forth.gr/~vmanous/">Victoria Manousaki</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://bouclas.github.io/">Konstantinos Bacharidis</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="http://users.ics.forth.gr/~papoutsa/">Konstantinos Papoutsakis</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://users.ics.forth.gr/~argyros/index.html">Antonis Argyros</a><sup>1,2</sup>,
            </span>
            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Computer Science Department, University of Crete, Greece</span>
            <span class="author-block"><sup>2</sup>Institute of Computer Science, FORTH, Greece</span>
			<span class="author-block"><sup>3</sup>Department of Management, Science & Technology, Hellenic Mediterranean University, Greece</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary (Soon)</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video (Soon)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Although existing methods for action anticipation have
			shown considerably improved performance on the predictability of future events in videos, the way they exploit 
			information related to past actions is constrained by time duration and encoding complexity. This paper addresses the
			task of action anticipation by taking into consideration the
			history of all executed actions throughout long, procedural activities. A novel approach noted as Visual-Linguistic
			Modeling of Action History (VLMAH) is proposed that fuses
			the immediate past in the form of visual features as well as
			the distant past based on a cost-effective form of linguistic
			constructs (semantic labels of the nouns, verbs, or actions).
			Our approach generates accurate near-future action predictions during procedural activities by leveraging 
			information on the long- and short-term past. Extensive experimental evaluation was conducted on three challenging video
			datasets containing procedural activities, namely the Meccano, the Assembly-101, and the 50Salads. The obtained
			results validate the importance of incorporating long-term
			action history for action anticipation and document the significant improvement of the state-of-the-art Top-1 accuracy
			performance.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

   
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Definition</h2>
        <div class="content has-text-justified">
<br/>
        <!-- Interpolating. -->
    
        
  
            <img src="./static/images/probdef.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
    
		<div class="content has-text-justified">
          <p>
           We consider the problem of action anticipation in
			untrimmed videos of procedural activities. At a certain moment
			in time (decision point), the proposed framework (VLMAH) an-
			ticipates the action (i.e., the unobserved action “take screw”) that
			is most likely to be performed after some anticipation time Tant
			(depicted with orange color). This is performed on the basis of the
			history of all past actions up to the decision point (depicted with
			purple) which is modeled by integrating visual input regarding the
			immediate past and a linguistic description of the distant past.
          </p>
      </div>
      </div>
    </div>
    <!--/ Animation. -->


  </div>
</section>







<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
<br/>
        <!-- Interpolating. -->
    
        
  
            <img src="./static/images/ACVR.PNG"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
    
		<div class="content has-text-justified">
          <p>
           The proposed VLMAH architecture. The Visual Action module and the Linguistic Action History module are presented. For the
Meccano dataset, the encoders of the action module, generate Object, Hands, Gaze representations, whereas for the Assembly-101 dataset,
there is a single encoder network, TSM [27] while representations are split into 3 sub-sequences. The detail
level regarding the textual label descriptions is adaptable to the anticipation task at hand (action, motion motif (verb), or object (noun)).
The final format also includes two special labels (START, END) that indicate the start and end of the action history sequence.
          </p>
        </div>
        <br/>
		</div>
    </div>
    <!--/ Animation. -->
   


  </div>
</section>










<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">


        <!-- Visual Effects. -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="content has-text-justified">
          <p>
           
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/id_26.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
	  </div>
      <!--/ Visual Effects. -->
   


  </div>
</section>






<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{vlmah2023anticipation,
  author    = {Victoria Manousaki, Konstantinos Bacharidis, Konstantinos Papoutsakis, and Antonis Argyros},
  title     = {VLMAH: Visual-Linguistic Modeling of Action History for Effective Action Anticipation},
  journal   = {ACVR Workshop - ICCV},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The source code of this webpage is from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
